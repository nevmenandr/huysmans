{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6daa298c-27c6-46c8-b16e-e5fed7e6bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :(\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "269c51b8-8ec9-45e0-96ea-59e0598cd2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "base_dir = 'corpus_preprocessed'\n",
    "\n",
    "for target in ['aimait', 'terribles']:\n",
    "    target_label = 1 if target == 'aimait' else 0\n",
    "    \n",
    "    target_dir = os.path.join(base_dir, target)\n",
    "    \n",
    "    for author in os.listdir(target_dir):\n",
    "        author_dir = os.path.join(target_dir, author)\n",
    "        \n",
    "        if os.path.isdir(author_dir):\n",
    "            for filename in os.listdir(author_dir):\n",
    "                if filename.endswith('.txt'):\n",
    "                    file_path = os.path.join(author_dir, filename)\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        text = file.read()\n",
    "                    data.append({\n",
    "                        'target': target_label,\n",
    "                        'author': author,\n",
    "                        'title': filename[:-4],\n",
    "                        'text': text\n",
    "                    })\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b1a6f21-b2b7-4aba-9a9d-27262058feca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>prudentius</td>\n",
       "      <td>prud.psycho</td>\n",
       "      <td>praefatio senex fidelis primus credo uia abram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sidonius</td>\n",
       "      <td>sidonius3</td>\n",
       "      <td>epistula sidonius auitus salus multus uinculum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>sidonius</td>\n",
       "      <td>sidonius2</td>\n",
       "      <td>epistula sidonius ecdicio salus duo nunc parit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>sidonius</td>\n",
       "      <td>sidonius1</td>\n",
       "      <td>epistula sidonius constantio salus praecipio d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>sidonius</td>\n",
       "      <td>sidonius5</td>\n",
       "      <td>epistula sidonius petronius salus audio lectit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>0</td>\n",
       "      <td>tacitus</td>\n",
       "      <td>tac.ann15</td>\n",
       "      <td>interea rex parthi uologaeses cognosco corbulo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>0</td>\n",
       "      <td>tacitus</td>\n",
       "      <td>tac.ann11</td>\n",
       "      <td>ualerium asiaticum bis consul quondam adulter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0</td>\n",
       "      <td>tacitus</td>\n",
       "      <td>tac.ann12</td>\n",
       "      <td>caedes messalinae convulsus princeps domus ori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0</td>\n",
       "      <td>tacitus</td>\n",
       "      <td>tac.ann13</td>\n",
       "      <td>primus nouus principatus mors iunii silanus pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0</td>\n",
       "      <td>ammianus</td>\n",
       "      <td>ammianus_res_gestae</td>\n",
       "      <td>liber caput gallus caesar saeuitia emetior ins...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>898 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     target      author                title  \\\n",
       "0         1  prudentius          prud.psycho   \n",
       "1         1    sidonius            sidonius3   \n",
       "2         1    sidonius            sidonius2   \n",
       "3         1    sidonius            sidonius1   \n",
       "4         1    sidonius            sidonius5   \n",
       "..      ...         ...                  ...   \n",
       "893       0     tacitus            tac.ann15   \n",
       "894       0     tacitus            tac.ann11   \n",
       "895       0     tacitus            tac.ann12   \n",
       "896       0     tacitus            tac.ann13   \n",
       "897       0    ammianus  ammianus_res_gestae   \n",
       "\n",
       "                                                  text  \n",
       "0    praefatio senex fidelis primus credo uia abram...  \n",
       "1    epistula sidonius auitus salus multus uinculum...  \n",
       "2    epistula sidonius ecdicio salus duo nunc parit...  \n",
       "3    epistula sidonius constantio salus praecipio d...  \n",
       "4    epistula sidonius petronius salus audio lectit...  \n",
       "..                                                 ...  \n",
       "893  interea rex parthi uologaeses cognosco corbulo...  \n",
       "894  ualerium asiaticum bis consul quondam adulter ...  \n",
       "895  caedes messalinae convulsus princeps domus ori...  \n",
       "896  primus nouus principatus mors iunii silanus pr...  \n",
       "897  liber caput gallus caesar saeuitia emetior ins...  \n",
       "\n",
       "[898 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23dfaa19-1ba8-450a-84c0-04d4f87d1b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import statistics\n",
    "\n",
    "def compute_accuracy(vocab, ngram, model, test_list, df, l2=False):\n",
    "    \n",
    "    accs = []\n",
    "    \n",
    "    for author in test_list:\n",
    "        \n",
    "        train_df = df[df['author'] != author]\n",
    "        test_df = df[df['author'] == author]\n",
    "        \n",
    "        less = min(train_df.target.value_counts())\n",
    "        balanced_df = pd.concat((train_df[train_df['target'] == 0].sample(less, random_state=8), train_df[train_df['target'] != 0].sample(less, random_state=8)))\n",
    "        \n",
    "        tfidf = TfidfVectorizer(max_features=vocab, ngram_range=(1, ngram))\n",
    "\n",
    "        X_train = tfidf.fit_transform(balanced_df['text']).toarray()\n",
    "        if l2:\n",
    "            X_train = normalize(X_train, norm='l2')\n",
    "        y_train = np.asarray(balanced_df['target'])\n",
    "        \n",
    "        X_test = tfidf.transform(test_df['text']).toarray()\n",
    "        if l2:\n",
    "            X_test = normalize(X_test, norm='l2')   \n",
    "        y_test = np.asarray(test_df['target'])\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pre = model.predict(X_test)\n",
    "        accs.append(accuracy_score(y_test,y_pre))\n",
    "    \n",
    "    return statistics.fmean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b80e01c-ef09-4f06-aaf3-c8d65c07d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dim = [500, 1000, 5000, 10000, 20000, 30000, 50000, 100000]\n",
    "ngram = [1, 2, 3, 4]\n",
    "models = [] # logreg, bernoulli NB, SVM, random forest, stacking (from process), extra trees, KNN Bonus: neural networkss (LSTM) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db845d78-c747-4b05-a7f7-c4c56ef11126",
   "metadata": {},
   "source": [
    "Принцип оценки качества моделей: выбираем случайно несколько авторов. Для каждого автора тренируем модель на всех остальных авторах, применяем модель на текстах выбранного автора, считаем среднее арифметическое по accuracy. итоговая accuracy это среднее по всем выбранным авторам.\n",
    "\n",
    "Для чистоты эксперимента выберем авторов для валидации заранее, одних и тех же для всех моделей. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b57903ff-f649-454a-98ec-0c6bb0e0cd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - target: 34\n",
      "1 - target: 32\n"
     ]
    }
   ],
   "source": [
    "authors_class_0 = df[df['target'] == 0]['author'].unique()\n",
    "authors_class_1 = df[df['target'] == 1]['author'].unique()\n",
    "\n",
    "print(f\"0 - target: {len(authors_class_0)}\")\n",
    "print(f\"1 - target: {len(authors_class_1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed66ae2-2de9-4d0f-ba12-4245e7d656f4",
   "metadata": {},
   "source": [
    "Выберем по 6 авторов для валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a432dbbf-78e4-4fa4-b461-81804b0e4a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - target: ['statius', 'gellius', 'alcuin', 'lactantius', 'plinius', 'arnobius']\n",
      "1 - target: ['gregorytours', 'petronius', 'tertullianus', 'orientus', 'pauldeacon', 'floridus']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(8)\n",
    "\n",
    "selected_authors_0 = random.sample(list(authors_class_0), 6)\n",
    "selected_authors_1 = random.sample(list(authors_class_1), 6)\n",
    "\n",
    "print(\"0 - target:\", selected_authors_0)\n",
    "print(\"1 - target:\", selected_authors_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "183006db-48ae-4b7b-a6ef-03fb627af893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of 0 - target val texts: 69\n",
      "number of 1 - target val texts: 38\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of 0 - target val texts: {df[df['author'].isin(selected_authors_0)].shape[0]}\")\n",
    "print(f\"number of 1 - target val texts: {df[df['author'].isin(selected_authors_1)].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf17fcf7-879a-4053-8eca-84468af70fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = selected_authors_0 + selected_authors_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1595a3-40d2-475c-860a-6f30d153edfe",
   "metadata": {},
   "source": [
    "**Оценка моделей**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc268ed8-c158-4c64-bd45-fa740aab5c69",
   "metadata": {},
   "source": [
    "Начнем по порядку оценивать качество моделей, варьируя гиперпараметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9f5e2b7-2caf-4b67-b5e8-e9a55d4aa6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "073a5825-f2d6-4176-89d7-f7783324a463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 500, max ngram: 1, accuracy: 0.7122120046849149\n",
      "Vocabulary size: 500, max ngram: 2, accuracy: 0.7122120046849149\n",
      "Vocabulary size: 500, max ngram: 3, accuracy: 0.7122120046849149\n",
      "Vocabulary size: 500, max ngram: 4, accuracy: 0.7122120046849149\n",
      "Vocabulary size: 1000, max ngram: 1, accuracy: 0.7207412935934608\n",
      "Vocabulary size: 1000, max ngram: 2, accuracy: 0.7172690713712385\n",
      "Vocabulary size: 1000, max ngram: 3, accuracy: 0.7172690713712385\n",
      "Vocabulary size: 1000, max ngram: 4, accuracy: 0.7172690713712385\n",
      "Vocabulary size: 5000, max ngram: 1, accuracy: 0.7402918038560453\n",
      "Vocabulary size: 5000, max ngram: 2, accuracy: 0.7310325445967861\n",
      "Vocabulary size: 5000, max ngram: 3, accuracy: 0.7310325445967861\n",
      "Vocabulary size: 5000, max ngram: 4, accuracy: 0.7310325445967861\n",
      "Vocabulary size: 10000, max ngram: 1, accuracy: 0.7266465796845054\n",
      "Vocabulary size: 10000, max ngram: 2, accuracy: 0.7231743574622831\n",
      "Vocabulary size: 10000, max ngram: 3, accuracy: 0.7231743574622831\n",
      "Vocabulary size: 10000, max ngram: 4, accuracy: 0.7231743574622831\n",
      "Vocabulary size: 20000, max ngram: 1, accuracy: 0.7315485404688191\n",
      "Vocabulary size: 20000, max ngram: 2, accuracy: 0.7182723966779694\n",
      "Vocabulary size: 20000, max ngram: 3, accuracy: 0.7182723966779694\n",
      "Vocabulary size: 20000, max ngram: 4, accuracy: 0.7182723966779694\n",
      "Vocabulary size: 30000, max ngram: 1, accuracy: 0.7315485404688191\n",
      "Vocabulary size: 30000, max ngram: 2, accuracy: 0.7246040960243746\n",
      "Vocabulary size: 30000, max ngram: 3, accuracy: 0.7280763182465969\n",
      "Vocabulary size: 30000, max ngram: 4, accuracy: 0.7231743574622831\n",
      "Vocabulary size: 50000, max ngram: 1, accuracy: 0.7315485404688191\n",
      "Vocabulary size: 50000, max ngram: 2, accuracy: 0.721958593378872\n",
      "Vocabulary size: 50000, max ngram: 3, accuracy: 0.7197021352400609\n",
      "Vocabulary size: 50000, max ngram: 4, accuracy: 0.7231743574622831\n",
      "Vocabulary size: 100000, max ngram: 1, accuracy: 0.7500670589873376\n",
      "Vocabulary size: 100000, max ngram: 2, accuracy: 0.7175726284665913\n",
      "Vocabulary size: 100000, max ngram: 3, accuracy: 0.7126706676822776\n",
      "Vocabulary size: 100000, max ngram: 4, accuracy: 0.7077687068979639\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "logreg_list = []\n",
    "\n",
    "for vocab in vocab_dim:\n",
    "    for n in ngram:\n",
    "\n",
    "        accuracy = compute_accuracy(vocab, n, model, test_list, df)\n",
    "        \n",
    "        print(f\"Vocabulary size: {vocab}, max ngram: {n}, accuracy: {accuracy}\")\n",
    "        logreg_list.append((vocab, n, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506a3b0-b825-46d2-85b4-02bef9d7ecf2",
   "metadata": {},
   "source": [
    "best: (100000, 1), (5000, 1), (50000, 1)\n",
    "OK: all\n",
    "\n",
    "при увеличении размера н-грамм результаты всегда ухудшаются, поэтому для этой модели имеет смысл рассматривать только 1-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bbcdc3b5-f707-41a3-a9f7-26e9791a7ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 500, max ngram: 1, accuracy: 0.6827372761970286\n",
      "Vocabulary size: 500, max ngram: 2, accuracy: 0.6827372761970286\n",
      "Vocabulary size: 500, max ngram: 3, accuracy: 0.6827372761970286\n",
      "Vocabulary size: 500, max ngram: 4, accuracy: 0.6827372761970286\n",
      "Vocabulary size: 1000, max ngram: 1, accuracy: 0.7237512695136533\n",
      "Vocabulary size: 1000, max ngram: 2, accuracy: 0.7237512695136533\n",
      "Vocabulary size: 1000, max ngram: 3, accuracy: 0.7237512695136533\n",
      "Vocabulary size: 1000, max ngram: 4, accuracy: 0.7237512695136533\n",
      "Vocabulary size: 5000, max ngram: 1, accuracy: 0.7299299105607155\n",
      "Vocabulary size: 5000, max ngram: 2, accuracy: 0.7189388913460121\n",
      "Vocabulary size: 5000, max ngram: 3, accuracy: 0.7189388913460121\n",
      "Vocabulary size: 5000, max ngram: 4, accuracy: 0.7224111135682344\n",
      "Vocabulary size: 10000, max ngram: 1, accuracy: 0.7299299105607155\n",
      "Vocabulary size: 10000, max ngram: 2, accuracy: 0.725056616213737\n",
      "Vocabulary size: 10000, max ngram: 3, accuracy: 0.725056616213737\n",
      "Vocabulary size: 10000, max ngram: 4, accuracy: 0.725056616213737\n",
      "Vocabulary size: 20000, max ngram: 1, accuracy: 0.7128124641669533\n",
      "Vocabulary size: 20000, max ngram: 2, accuracy: 0.7128124641669533\n",
      "Vocabulary size: 20000, max ngram: 3, accuracy: 0.7128124641669533\n",
      "Vocabulary size: 20000, max ngram: 4, accuracy: 0.7128124641669533\n",
      "Vocabulary size: 30000, max ngram: 1, accuracy: 0.7128124641669533\n",
      "Vocabulary size: 30000, max ngram: 2, accuracy: 0.7128124641669533\n",
      "Vocabulary size: 30000, max ngram: 3, accuracy: 0.7128124641669533\n",
      "Vocabulary size: 30000, max ngram: 4, accuracy: 0.7128124641669533\n",
      "Vocabulary size: 50000, max ngram: 1, accuracy: 0.7128124641669533\n",
      "Vocabulary size: 50000, max ngram: 2, accuracy: 0.7049542770324505\n",
      "Vocabulary size: 50000, max ngram: 3, accuracy: 0.7049542770324505\n",
      "Vocabulary size: 50000, max ngram: 4, accuracy: 0.7049542770324505\n",
      "Vocabulary size: 100000, max ngram: 1, accuracy: 0.7220717234262125\n",
      "Vocabulary size: 100000, max ngram: 2, accuracy: 0.772484212983439\n",
      "Vocabulary size: 100000, max ngram: 3, accuracy: 0.697922809474667\n",
      "Vocabulary size: 100000, max ngram: 4, accuracy: 0.697922809474667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "model = svm.SVC()\n",
    "svm_list = []\n",
    "\n",
    "for vocab in vocab_dim:\n",
    "    for n in ngram:\n",
    "\n",
    "        accuracy = compute_accuracy(vocab, n, model, test_list, df)\n",
    "        \n",
    "        print(f\"Vocabulary size: {vocab}, max ngram: {n}, accuracy: {accuracy}\")\n",
    "        svm_list.append((vocab, n, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f863021a-9b46-4757-978c-615d0b75c83b",
   "metadata": {},
   "source": [
    "best: (100000, 2), (10000, 1), (10000, 2-4)\n",
    "OK: vocab: 1000-30000, (100000, 1-2), (50000, 1)\n",
    "\n",
    "Для этой модели тоже будем рассматривать только 1-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7674a52-ab58-4939-8225-f6eb37ba4a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 500, max ngram: 1, accuracy: 0.7384530566612609\n",
      "Vocabulary size: 500, max ngram: 2, accuracy: 0.6753490138745556\n",
      "Vocabulary size: 500, max ngram: 3, accuracy: 0.7009983086801972\n",
      "Vocabulary size: 500, max ngram: 4, accuracy: 0.756173522040395\n",
      "Vocabulary size: 1000, max ngram: 1, accuracy: 0.6949066907464739\n",
      "Vocabulary size: 1000, max ngram: 2, accuracy: 0.657317824790735\n",
      "Vocabulary size: 1000, max ngram: 3, accuracy: 0.6456577514046554\n",
      "Vocabulary size: 1000, max ngram: 4, accuracy: 0.6690173145281504\n",
      "Vocabulary size: 5000, max ngram: 1, accuracy: 0.6960457721100136\n",
      "Vocabulary size: 5000, max ngram: 2, accuracy: 0.6536506707946336\n",
      "Vocabulary size: 5000, max ngram: 3, accuracy: 0.6621624527003783\n",
      "Vocabulary size: 5000, max ngram: 4, accuracy: 0.655734004127967\n",
      "Vocabulary size: 10000, max ngram: 1, accuracy: 0.7081358420561207\n",
      "Vocabulary size: 10000, max ngram: 2, accuracy: 0.655953302373581\n",
      "Vocabulary size: 10000, max ngram: 3, accuracy: 0.6792974060969418\n",
      "Vocabulary size: 10000, max ngram: 4, accuracy: 0.5626254156633413\n",
      "Vocabulary size: 20000, max ngram: 1, accuracy: 0.6645059339525284\n",
      "Vocabulary size: 20000, max ngram: 2, accuracy: 0.6535324217406261\n",
      "Vocabulary size: 20000, max ngram: 3, accuracy: 0.6592062263501892\n",
      "Vocabulary size: 20000, max ngram: 4, accuracy: 0.6579749004865104\n",
      "Vocabulary size: 30000, max ngram: 1, accuracy: 0.6433716030271758\n",
      "Vocabulary size: 30000, max ngram: 2, accuracy: 0.6240263649319376\n",
      "Vocabulary size: 30000, max ngram: 3, accuracy: 0.6870807533539732\n",
      "Vocabulary size: 30000, max ngram: 4, accuracy: 0.6543042655658754\n",
      "Vocabulary size: 50000, max ngram: 1, accuracy: 0.6650219298245614\n",
      "Vocabulary size: 50000, max ngram: 2, accuracy: 0.6444035947712418\n",
      "Vocabulary size: 50000, max ngram: 3, accuracy: 0.6904562263501891\n",
      "Vocabulary size: 50000, max ngram: 4, accuracy: 0.6429738562091504\n",
      "Vocabulary size: 100000, max ngram: 1, accuracy: 0.6885104919160647\n",
      "Vocabulary size: 100000, max ngram: 2, accuracy: 0.6282679738562091\n",
      "Vocabulary size: 100000, max ngram: 3, accuracy: 0.6355134158926729\n",
      "Vocabulary size: 100000, max ngram: 4, accuracy: 0.6552180082559339\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "forest_list = []\n",
    "\n",
    "\n",
    "for vocab in vocab_dim:\n",
    "    for n in ngram:\n",
    "\n",
    "        accuracy = compute_accuracy(vocab, n, model, test_list, df)\n",
    "        \n",
    "        print(f\"Vocabulary size: {vocab}, max ngram: {n}, accuracy: {accuracy}\")\n",
    "        forest_list.append((vocab, n, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7fa24e-a96a-475a-92ed-f6fa09c7c815",
   "metadata": {},
   "source": [
    "best: (100, 4), (50, 3), (100, 2), (500, 4)\n",
    "OK: vocab: 500, 50, 100\n",
    "\n",
    "результаты местами хорошие, но не очень стабильные. общей зависимости качества от размера н-грамм нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "777f13d0-80d9-4df7-88bf-748849e99037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50, max ngram: 1, accuracy: 0.7491744066047472\n",
      "Vocabulary size: 50, max ngram: 2, accuracy: 0.7396306329549364\n",
      "Vocabulary size: 50, max ngram: 3, accuracy: 0.774828001375989\n",
      "Vocabulary size: 50, max ngram: 4, accuracy: 0.7265562804068996\n",
      "Vocabulary size: 100, max ngram: 1, accuracy: 0.7217499426671253\n",
      "Vocabulary size: 100, max ngram: 2, accuracy: 0.7645539502350648\n",
      "Vocabulary size: 100, max ngram: 3, accuracy: 0.7458813496158697\n",
      "Vocabulary size: 100, max ngram: 4, accuracy: 0.7773262813897489\n",
      "Vocabulary size: 300, max ngram: 1, accuracy: 0.5894295583730568\n",
      "Vocabulary size: 300, max ngram: 2, accuracy: 0.6715757940603142\n",
      "Vocabulary size: 300, max ngram: 3, accuracy: 0.609493505004341\n",
      "Vocabulary size: 300, max ngram: 4, accuracy: 0.6833039502350647\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "\n",
    "for vocab in [50, 100, 300]:\n",
    "    for n in ngram:\n",
    "\n",
    "        accuracy = compute_accuracy(vocab, n, model, test_list, df)\n",
    "        \n",
    "        print(f\"Vocabulary size: {vocab}, max ngram: {n}, accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70456209-1ace-4513-b2a5-72a5efd88443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 500, max ngram: 1, accuracy: 0.5590972938883155\n",
      "Vocabulary size: 500, max ngram: 2, accuracy: 0.5590972938883155\n",
      "Vocabulary size: 500, max ngram: 3, accuracy: 0.5590972938883155\n",
      "Vocabulary size: 500, max ngram: 4, accuracy: 0.5590972938883155\n",
      "Vocabulary size: 1000, max ngram: 1, accuracy: 0.591777032450407\n",
      "Vocabulary size: 1000, max ngram: 2, accuracy: 0.591777032450407\n",
      "Vocabulary size: 1000, max ngram: 3, accuracy: 0.591777032450407\n",
      "Vocabulary size: 1000, max ngram: 4, accuracy: 0.591777032450407\n",
      "Vocabulary size: 5000, max ngram: 1, accuracy: 0.6667225662194703\n",
      "Vocabulary size: 5000, max ngram: 2, accuracy: 0.6722781217750259\n",
      "Vocabulary size: 5000, max ngram: 3, accuracy: 0.6722781217750259\n",
      "Vocabulary size: 5000, max ngram: 4, accuracy: 0.6722781217750259\n",
      "Vocabulary size: 10000, max ngram: 1, accuracy: 0.6404153766769866\n",
      "Vocabulary size: 10000, max ngram: 2, accuracy: 0.6404153766769866\n",
      "Vocabulary size: 10000, max ngram: 3, accuracy: 0.6404153766769866\n",
      "Vocabulary size: 10000, max ngram: 4, accuracy: 0.6404153766769866\n",
      "Vocabulary size: 20000, max ngram: 1, accuracy: 0.6770434050813309\n",
      "Vocabulary size: 20000, max ngram: 2, accuracy: 0.6854175880878667\n",
      "Vocabulary size: 20000, max ngram: 3, accuracy: 0.6898035530001474\n",
      "Vocabulary size: 20000, max ngram: 4, accuracy: 0.6854175880878667\n",
      "Vocabulary size: 30000, max ngram: 1, accuracy: 0.6872450734679837\n",
      "Vocabulary size: 30000, max ngram: 2, accuracy: 0.6951032606024866\n",
      "Vocabulary size: 30000, max ngram: 3, accuracy: 0.6994892255147672\n",
      "Vocabulary size: 30000, max ngram: 4, accuracy: 0.6994892255147672\n",
      "Vocabulary size: 50000, max ngram: 1, accuracy: 0.777157354169738\n",
      "Vocabulary size: 50000, max ngram: 2, accuracy: 0.7969203032090029\n",
      "Vocabulary size: 50000, max ngram: 3, accuracy: 0.7978340458990614\n",
      "Vocabulary size: 50000, max ngram: 4, accuracy: 0.7978340458990614\n",
      "Vocabulary size: 100000, max ngram: 1, accuracy: 0.7996615312791784\n",
      "Vocabulary size: 100000, max ngram: 2, accuracy: 0.8249007936507936\n",
      "Vocabulary size: 100000, max ngram: 3, accuracy: 0.8249007936507936\n",
      "Vocabulary size: 100000, max ngram: 4, accuracy: 0.8249007936507936\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "model = BernoulliNB()\n",
    "bernoulli_list = []\n",
    "\n",
    "for vocab in vocab_dim:\n",
    "    for n in ngram:\n",
    "\n",
    "        accuracy = compute_accuracy(vocab, n, model, test_list, df)\n",
    "        \n",
    "        print(f\"Vocabulary size: {vocab}, max ngram: {n}, accuracy: {accuracy}\")\n",
    "        bernoulli_list.append((vocab, n, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ead9a0a-62e1-4a5e-9c23-b2102f0e69eb",
   "metadata": {},
   "source": [
    "best: (100000, 2-3), (100000, 1), (50000, 3-4)\n",
    "OK: vocab: 50000-100000\n",
    "\n",
    "Здесь явно улучшается качество при увеличении размера н-грамм, будем рассмотривать только 4-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "096c58da-bdec-4586-a5b2-cef94c55e3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 500, max ngram: 1, accuracy: 0.6238905064949957\n",
      "Vocabulary size: 500, max ngram: 2, accuracy: 0.6238905064949957\n",
      "Vocabulary size: 500, max ngram: 3, accuracy: 0.6238905064949957\n",
      "Vocabulary size: 500, max ngram: 4, accuracy: 0.6238905064949957\n",
      "Vocabulary size: 1000, max ngram: 1, accuracy: 0.618972881550281\n",
      "Vocabulary size: 1000, max ngram: 2, accuracy: 0.618972881550281\n",
      "Vocabulary size: 1000, max ngram: 3, accuracy: 0.618972881550281\n",
      "Vocabulary size: 1000, max ngram: 4, accuracy: 0.618972881550281\n",
      "Vocabulary size: 5000, max ngram: 1, accuracy: 0.6390702041050339\n",
      "Vocabulary size: 5000, max ngram: 2, accuracy: 0.6274717635592248\n",
      "Vocabulary size: 5000, max ngram: 3, accuracy: 0.6330273191147804\n",
      "Vocabulary size: 5000, max ngram: 4, accuracy: 0.6330273191147804\n",
      "Vocabulary size: 10000, max ngram: 1, accuracy: 0.6090184611856438\n",
      "Vocabulary size: 10000, max ngram: 2, accuracy: 0.6420221304896228\n",
      "Vocabulary size: 10000, max ngram: 3, accuracy: 0.6315646141497534\n",
      "Vocabulary size: 10000, max ngram: 4, accuracy: 0.6315646141497534\n",
      "Vocabulary size: 20000, max ngram: 1, accuracy: 0.6620972365554408\n",
      "Vocabulary size: 20000, max ngram: 2, accuracy: 0.6384818254787296\n",
      "Vocabulary size: 20000, max ngram: 3, accuracy: 0.6628282307074876\n",
      "Vocabulary size: 20000, max ngram: 4, accuracy: 0.6628282307074876\n",
      "Vocabulary size: 30000, max ngram: 1, accuracy: 0.6729159500057333\n",
      "Vocabulary size: 30000, max ngram: 2, accuracy: 0.6936561174177274\n",
      "Vocabulary size: 30000, max ngram: 3, accuracy: 0.6985580782020411\n",
      "Vocabulary size: 30000, max ngram: 4, accuracy: 0.6936561174177274\n",
      "Vocabulary size: 50000, max ngram: 1, accuracy: 0.6158984061460842\n",
      "Vocabulary size: 50000, max ngram: 2, accuracy: 0.6985867446393762\n",
      "Vocabulary size: 50000, max ngram: 3, accuracy: 0.7010635248251348\n",
      "Vocabulary size: 50000, max ngram: 4, accuracy: 0.7103514505217291\n",
      "Vocabulary size: 100000, max ngram: 1, accuracy: 0.6986608678559142\n",
      "Vocabulary size: 100000, max ngram: 2, accuracy: 0.7041423001949317\n",
      "Vocabulary size: 100000, max ngram: 3, accuracy: 0.7041423001949317\n",
      "Vocabulary size: 100000, max ngram: 4, accuracy: 0.7041423001949317\n"
     ]
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "gaussian_list = []\n",
    "\n",
    "for vocab in vocab_dim:\n",
    "    for n in ngram:\n",
    "\n",
    "        accuracy = compute_accuracy(vocab, n, model, test_list, df)\n",
    "        \n",
    "        print(f\"Vocabulary size: {vocab}, max ngram: {n}, accuracy: {accuracy}\")\n",
    "        gaussian_list.append((vocab, n, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a829b2-4c7b-476a-aa3a-85797660378f",
   "metadata": {},
   "source": [
    "best: (500000, 4)\n",
    "OK: vocab>=50000, n>=3\n",
    "\n",
    "плохая точность, выкинем из рассмотрения эту модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6afec6e9-26db-43b9-9fbe-8f7e6e2a3600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 500, max ngram: 1, accuracy: 0.6762453519419463\n",
      "Vocabulary size: 500, max ngram: 2, accuracy: 0.6599546251249038\n",
      "Vocabulary size: 500, max ngram: 3, accuracy: 0.6599546251249038\n",
      "Vocabulary size: 500, max ngram: 4, accuracy: 0.6599546251249038\n",
      "Vocabulary size: 1000, max ngram: 1, accuracy: 0.7110458949661735\n",
      "Vocabulary size: 1000, max ngram: 2, accuracy: 0.7061439341818598\n",
      "Vocabulary size: 1000, max ngram: 3, accuracy: 0.7061439341818598\n",
      "Vocabulary size: 1000, max ngram: 4, accuracy: 0.7061439341818598\n",
      "Vocabulary size: 5000, max ngram: 1, accuracy: 0.7092184095860566\n",
      "Vocabulary size: 5000, max ngram: 2, accuracy: 0.7092184095860566\n",
      "Vocabulary size: 5000, max ngram: 3, accuracy: 0.7092184095860566\n",
      "Vocabulary size: 5000, max ngram: 4, accuracy: 0.7092184095860566\n",
      "Vocabulary size: 10000, max ngram: 1, accuracy: 0.7077303143479613\n",
      "Vocabulary size: 10000, max ngram: 2, accuracy: 0.6685438064114534\n",
      "Vocabulary size: 10000, max ngram: 3, accuracy: 0.7042580921257392\n",
      "Vocabulary size: 10000, max ngram: 4, accuracy: 0.7042580921257392\n",
      "Vocabulary size: 20000, max ngram: 1, accuracy: 0.5932053376906318\n",
      "Vocabulary size: 20000, max ngram: 2, accuracy: 0.6299700435729848\n",
      "Vocabulary size: 20000, max ngram: 3, accuracy: 0.6299700435729848\n",
      "Vocabulary size: 20000, max ngram: 4, accuracy: 0.6299700435729848\n",
      "Vocabulary size: 30000, max ngram: 1, accuracy: 0.6716367102396514\n",
      "Vocabulary size: 30000, max ngram: 2, accuracy: 0.6667347494553377\n",
      "Vocabulary size: 30000, max ngram: 3, accuracy: 0.6667347494553377\n",
      "Vocabulary size: 30000, max ngram: 4, accuracy: 0.6667347494553377\n",
      "Vocabulary size: 50000, max ngram: 1, accuracy: 0.6667347494553377\n",
      "Vocabulary size: 50000, max ngram: 2, accuracy: 0.6667347494553377\n",
      "Vocabulary size: 50000, max ngram: 3, accuracy: 0.6667347494553377\n",
      "Vocabulary size: 50000, max ngram: 4, accuracy: 0.6667347494553377\n",
      "Vocabulary size: 100000, max ngram: 1, accuracy: 0.6667347494553377\n",
      "Vocabulary size: 100000, max ngram: 2, accuracy: 0.6618327886710239\n",
      "Vocabulary size: 100000, max ngram: 3, accuracy: 0.6618327886710239\n",
      "Vocabulary size: 100000, max ngram: 4, accuracy: 0.6618327886710239\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "multinomial_list = []\n",
    "\n",
    "for vocab in vocab_dim:\n",
    "    for n in ngram:\n",
    "\n",
    "        accuracy = compute_accuracy(vocab, n, model, test_list, df)\n",
    "        \n",
    "        print(f\"Vocabulary size: {vocab}, max ngram: {n}, accuracy: {accuracy}\")\n",
    "        multinomial_list.append((vocab, n, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3db7712-b960-4346-879d-2bd3120b5abd",
   "metadata": {},
   "source": [
    "плохая точность, тоже не будем рассматривать эту модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9005649e-751a-4459-88d0-df9abcc01d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 500, max ngram: 1, accuracy: 0.6450711132406178\n",
      "Vocabulary size: 500, max ngram: 2, accuracy: 0.6756746850787098\n",
      "Vocabulary size: 500, max ngram: 3, accuracy: 0.6997974920962537\n",
      "Vocabulary size: 500, max ngram: 4, accuracy: 0.6581308254295871\n",
      "Vocabulary size: 1000, max ngram: 1, accuracy: 0.7111600488148476\n",
      "Vocabulary size: 1000, max ngram: 2, accuracy: 0.6231484552885481\n",
      "Vocabulary size: 1000, max ngram: 3, accuracy: 0.7237698003177879\n",
      "Vocabulary size: 1000, max ngram: 4, accuracy: 0.6280217496355267\n",
      "Vocabulary size: 5000, max ngram: 1, accuracy: 0.7028146346257801\n",
      "Vocabulary size: 5000, max ngram: 2, accuracy: 0.6926056972496601\n",
      "Vocabulary size: 5000, max ngram: 3, accuracy: 0.7156147107965993\n",
      "Vocabulary size: 5000, max ngram: 4, accuracy: 0.6006345520664406\n",
      "Vocabulary size: 10000, max ngram: 1, accuracy: 0.636530664897538\n",
      "Vocabulary size: 10000, max ngram: 2, accuracy: 0.7159940291906236\n",
      "Vocabulary size: 10000, max ngram: 3, accuracy: 0.6986713106295149\n",
      "Vocabulary size: 10000, max ngram: 4, accuracy: 0.7276848166167053\n",
      "Vocabulary size: 20000, max ngram: 1, accuracy: 0.6959970391665439\n",
      "Vocabulary size: 20000, max ngram: 2, accuracy: 0.722202258096221\n",
      "Vocabulary size: 20000, max ngram: 3, accuracy: 0.7130804953560371\n",
      "Vocabulary size: 20000, max ngram: 4, accuracy: 0.6698056415548675\n",
      "Vocabulary size: 30000, max ngram: 1, accuracy: 0.6677058250200666\n",
      "Vocabulary size: 30000, max ngram: 2, accuracy: 0.6922689689911051\n",
      "Vocabulary size: 30000, max ngram: 3, accuracy: 0.73484323553983\n",
      "Vocabulary size: 30000, max ngram: 4, accuracy: 0.7007789080544499\n",
      "Vocabulary size: 50000, max ngram: 1, accuracy: 0.6318304953560371\n",
      "Vocabulary size: 50000, max ngram: 2, accuracy: 0.6583508403361344\n",
      "Vocabulary size: 50000, max ngram: 3, accuracy: 0.6840713917145805\n",
      "Vocabulary size: 50000, max ngram: 4, accuracy: 0.7255961595164382\n",
      "Vocabulary size: 100000, max ngram: 1, accuracy: 0.6918051869870755\n",
      "Vocabulary size: 100000, max ngram: 2, accuracy: 0.7127776549216178\n",
      "Vocabulary size: 100000, max ngram: 3, accuracy: 0.7148784952577523\n",
      "Vocabulary size: 100000, max ngram: 4, accuracy: 0.6781137893753993\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "model = ExtraTreesClassifier(n_estimators=50, random_state=2)\n",
    "trees_list = []\n",
    "\n",
    "for vocab in vocab_dim:\n",
    "    for n in ngram:\n",
    "\n",
    "        accuracy = compute_accuracy(vocab, n, model, test_list, df)\n",
    "        \n",
    "        print(f\"Vocabulary size: {vocab}, max ngram: {n}, accuracy: {accuracy}\")\n",
    "        trees_list.append((vocab, n, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad8e6d6-57a3-4b69-8047-9b984737dc9b",
   "metadata": {},
   "source": [
    "Результаты местами неплохие, но нестабильные\n",
    "\n",
    "best: (30000, 3), (10000, 4), (50000, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6dbce8bb-171d-41c9-9bc9-433ef96d9f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3 neighbors:\n",
      "Vocabulary size: 100, max ngram: 1, accuracy: 0.7612449219453863\n",
      "Vocabulary size: 100, max ngram: 2, accuracy: 0.7612449219453863\n",
      "Vocabulary size: 300, max ngram: 1, accuracy: 0.7151805166511048\n",
      "Vocabulary size: 300, max ngram: 2, accuracy: 0.7438920012449425\n",
      "Vocabulary size: 500, max ngram: 1, accuracy: 0.7286332661719658\n",
      "Vocabulary size: 500, max ngram: 2, accuracy: 0.7286332661719658\n",
      "Vocabulary size: 1000, max ngram: 1, accuracy: 0.7771071879044015\n",
      "Vocabulary size: 1000, max ngram: 2, accuracy: 0.7771071879044015\n",
      "Vocabulary size: 3000, max ngram: 1, accuracy: 0.70281094894098\n",
      "Vocabulary size: 3000, max ngram: 2, accuracy: 0.690906187036218\n",
      "Vocabulary size: 5000, max ngram: 1, accuracy: 0.7429111995675464\n",
      "Vocabulary size: 5000, max ngram: 2, accuracy: 0.7071969138532607\n",
      "Vocabulary size: 10000, max ngram: 1, accuracy: 0.7248800104837256\n",
      "Vocabulary size: 10000, max ngram: 2, accuracy: 0.7081019542319852\n",
      "\n",
      " 5 neighbors:\n",
      "Vocabulary size: 100, max ngram: 1, accuracy: 0.560526111029207\n",
      "Vocabulary size: 100, max ngram: 2, accuracy: 0.560526111029207\n",
      "Vocabulary size: 300, max ngram: 1, accuracy: 0.6489943199502023\n",
      "Vocabulary size: 300, max ngram: 2, accuracy: 0.6608990818549642\n",
      "Vocabulary size: 500, max ngram: 1, accuracy: 0.6440053360525496\n",
      "Vocabulary size: 500, max ngram: 2, accuracy: 0.6440053360525496\n",
      "Vocabulary size: 1000, max ngram: 1, accuracy: 0.7346721583370189\n",
      "Vocabulary size: 1000, max ngram: 2, accuracy: 0.7178654356479434\n",
      "Vocabulary size: 3000, max ngram: 1, accuracy: 0.6877943428833522\n",
      "Vocabulary size: 3000, max ngram: 2, accuracy: 0.68432212066113\n",
      "Vocabulary size: 5000, max ngram: 1, accuracy: 0.6212677322390945\n",
      "Vocabulary size: 5000, max ngram: 2, accuracy: 0.6177955100168723\n",
      "Vocabulary size: 10000, max ngram: 1, accuracy: 0.6954001629891723\n",
      "Vocabulary size: 10000, max ngram: 2, accuracy: 0.6024185258898881\n",
      "\n",
      " 10 neighbors:\n",
      "Vocabulary size: 100, max ngram: 1, accuracy: 0.6529252051697872\n",
      "Vocabulary size: 100, max ngram: 2, accuracy: 0.6529252051697872\n",
      "Vocabulary size: 300, max ngram: 1, accuracy: 0.6738547758284601\n",
      "Vocabulary size: 300, max ngram: 2, accuracy: 0.6689528150441463\n",
      "Vocabulary size: 500, max ngram: 1, accuracy: 0.7382518796992481\n",
      "Vocabulary size: 500, max ngram: 2, accuracy: 0.7382518796992481\n",
      "Vocabulary size: 1000, max ngram: 1, accuracy: 0.614204014939309\n",
      "Vocabulary size: 1000, max ngram: 2, accuracy: 0.614204014939309\n",
      "Vocabulary size: 3000, max ngram: 1, accuracy: 0.6515717398070339\n",
      "Vocabulary size: 3000, max ngram: 2, accuracy: 0.6480995175848118\n",
      "Vocabulary size: 5000, max ngram: 1, accuracy: 0.6666380002293315\n",
      "Vocabulary size: 5000, max ngram: 2, accuracy: 0.6437135526725309\n",
      "Vocabulary size: 10000, max ngram: 1, accuracy: 0.6622520353170508\n",
      "Vocabulary size: 10000, max ngram: 2, accuracy: 0.6503472734122889\n",
      "\n",
      " 15 neighbors:\n",
      "Vocabulary size: 100, max ngram: 1, accuracy: 0.6420150662604224\n",
      "Vocabulary size: 100, max ngram: 2, accuracy: 0.6420150662604224\n",
      "Vocabulary size: 300, max ngram: 1, accuracy: 0.7169609071698856\n",
      "Vocabulary size: 300, max ngram: 2, accuracy: 0.7239637082903337\n",
      "Vocabulary size: 500, max ngram: 1, accuracy: 0.6278118703621799\n",
      "Vocabulary size: 500, max ngram: 2, accuracy: 0.6278118703621799\n",
      "Vocabulary size: 1000, max ngram: 1, accuracy: 0.5560910036529232\n",
      "Vocabulary size: 1000, max ngram: 2, accuracy: 0.5343823201795338\n",
      "Vocabulary size: 3000, max ngram: 1, accuracy: 0.6313127590217373\n",
      "Vocabulary size: 3000, max ngram: 2, accuracy: 0.6370997960587744\n",
      "Vocabulary size: 5000, max ngram: 1, accuracy: 0.6357883065506905\n",
      "Vocabulary size: 5000, max ngram: 2, accuracy: 0.6332298270185267\n",
      "Vocabulary size: 10000, max ngram: 1, accuracy: 0.6398348813209495\n",
      "Vocabulary size: 10000, max ngram: 2, accuracy: 0.6415753435877275\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "for num in [3, 5, 10, 15]:\n",
    "    print(f'\\n {num} neighbors:')\n",
    "    model = KNeighborsClassifier(n_neighbors=num, metric='cosine')\n",
    "    for vocab in [100, 300, 500, 1000, 3000, 5000, 10000]:\n",
    "        for n in [1, 2]:\n",
    "    \n",
    "            accuracy = compute_accuracy(vocab, n, model, test_list, df)\n",
    "            \n",
    "            print(f\"Vocabulary size: {vocab}, max ngram: {n}, accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022071e6-9168-4c4c-995f-081ac39dd572",
   "metadata": {},
   "source": [
    "best: (3, 1000, 1-2), (3, 100, 1-2), (3, 5000, 1)\n",
    "OK: num of neighbors = 3\n",
    "\n",
    "будем рассматривать только 1-граммы для этой модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c62ce56-0449-4226-a438-b8e91bba8f34",
   "metadata": {},
   "source": [
    "Итого мы оставляем для дальнейшего исследования:\n",
    "\n",
    "1) логистическая регрессия: (5000-100000, 1)\n",
    "2) SVM: (5000-10000, 1)\n",
    "3) ~случайный лес: (1-4, 50-500)\n",
    "4) бернулли: (50000-100000, 4)\n",
    "5) KNN: (3, 100-1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af7ee25-e22a-429f-83b0-88e195ab148e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
